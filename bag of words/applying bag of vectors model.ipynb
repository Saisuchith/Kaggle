{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we dig a little \"deeper\" into sentiment analysis.\n",
    "\n",
    "Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficients\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About The data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "TestData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Our task is to predict the sentiment for each one. \n",
    "\n",
    "DATA FIELDS:\n",
    "\n",
    "id - Unique ID of each review\n",
    "\n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "\n",
    "review - Text of the review\n",
    "\n",
    "The data set is association with:\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "\n",
    "The data was taken from https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before starting on a machine learning task, we have to preprocess the data.\n",
    "\n",
    "• Word Stemming: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "\n",
    "• Removal of non-words: Non-words and punctuation have been re-\n",
    "moved. All white spaces (tabs, newlines, spaces) have all been trimmed\n",
    "to a single space character.\n",
    "\n",
    "## Vocabulary List\n",
    "After preprocessing the reviews, we have a list of words for\n",
    "each review. The next step is to choose which words we would like to use in\n",
    "our classifier and which we would want to leave out.\n",
    "For this exercise, I have chosen only the most frequently occuring words\n",
    "as our set of words considered (the vocabulary list).\n",
    "\n",
    "## Extracting Features\n",
    "We will now implement the feature extraction that converts each review into\n",
    "a vector . For this exercise, you will be using n = # words in vocabulary\n",
    "list. Specifically, the feature x(i) belongs {0, 1} for an review corresponds to whether\n",
    "the i-th word in the dictionary occurs in the reviews. That is, x(i) = 1 if the i-th\n",
    "word is in the review and x(i) = 0 if the i-th word is not present in the review. Here i have used n=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the file name starts with tsv. So the delimeter is a tab and quoting=3 implies it avoids doubles quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train=pd.read_csv(\"labeledTrainData.tsv\",delimiter='\\t',quoting=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start cleaning the text. Before that we will import some libraries and do some preprocessing and for \n",
    "removing stopwords i have used nltk(Natural Language Toolkit) library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/suchith/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords               # for removing the words like 'the' ,'a','an' from the set\n",
    "from nltk.stem.porter import PorterStemmer      # for making words like loved as love and loving as love       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "corpus=[]                         # this contains our reviews after cleaning \n",
    "for i in range(0,25000):          # we will iterate through each review and clean it\n",
    "    \n",
    "    # its for removing exclamtion marks, question marks and like this stuff \n",
    "    Review=re.sub('[^a-zA-Z]',' ',train['review'][i])       \n",
    "    \n",
    "    # making every letter to lower case\n",
    "    Review=Review.lower()\n",
    "    \n",
    "    # splitting the string in words( why we do this.. the reasons are below)\n",
    "    Review=Review.split()\n",
    "    \n",
    "    # we are removing the words like 'the','an'  and we are also stemming it \n",
    "    Review=[ps.stem(word) for word in Review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # after cleaning we are just joning to make it as string\n",
    "    Review=' '.join(Review)\n",
    "    \n",
    "    # we will do this for every review and the review we got after cleaning we are appending to corpus\n",
    "    corpus.append(Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned our data and our data is ready to build a bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using python inbuilt library for building the Bag of words model and i have taken 5000 most frequent words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=5000)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "y=train.iloc[:,1].values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000) (5000, 5000) (20000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_cross,y_train,y_cross=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "print(X_train.shape,X_cross.shape,y_train.shape,y_cross.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will a Random Forest Classifier and Naive Bayes Classifier as our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have fitted the model by X,y and not by X_train and y_train. It's because we know that our model overfits \n",
    "\n",
    "the data. So to increase the accuracy on test data we just want more data. So i fitted the model with X,y. And i \n",
    "\n",
    "have observed that accuracy increased from 84.24% to 84.26% That's so small I know that :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making predictions on Cross data set\n",
    "y_pred_rf=rf.predict(X_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2481    0]\n",
      " [   0 2519]]\n"
     ]
    }
   ],
   "source": [
    "# We will print the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm=confusion_matrix(y_pred_rf,y_cross)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc_rf=accuracy_score(y_pred_rf,y_cross)\n",
    "print(acc_rf)\n",
    "print(accuracy_score(rf.predict(X_train),y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Naive-Bayes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2133 1129]\n",
      " [ 348 1390]]\n",
      "0.7046\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb=nb.predict(X_cross)\n",
    "cm1=confusion_matrix(y_pred_nb,y_cross)\n",
    "print(cm1)\n",
    "acc_nb=accuracy_score(y_pred_nb,y_cross)\n",
    "print(acc_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose Random Forest Classifier as it's performance is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "           id                                             review\n",
      "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
      "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
      "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
      "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
      "4   \"12128_7\"  \"A very accurate depiction of small time mob l...\n",
      "5    \"2913_8\"  \"...as valuable as King Tut's tomb! (OK, maybe...\n",
      "6    \"4396_1\"  \"This has to be one of the biggest misfires ev...\n",
      "7     \"395_2\"  \"This is one of those movies I watched, and wo...\n",
      "8   \"10616_1\"  \"The worst movie i've seen in years (and i've ...\n",
      "9    \"9074_9\"  \"Five medical students (Kevin Bacon, David Lab...\n"
     ]
    }
   ],
   "source": [
    "test=pd.read_csv(\"testData.tsv\",delimiter='\\t',quoting=3)\n",
    "print(test.shape)\n",
    "print(test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps1=PorterStemmer()\n",
    "corpus1=[]                         # this contains our reviews after cleaning \n",
    "for i in range(0,25000):          # we will iterate through each review and clean it\n",
    "    \n",
    "    # its for removing exclamtion marks, question marks and like this stuff \n",
    "    Review=re.sub('[^a-zA-Z]',' ',test['review'][i])       \n",
    "    \n",
    "    # making every letter to lower case\n",
    "    Review=Review.lower()\n",
    "    \n",
    "    # splitting the string in words( why we do this.. the reasons are below)\n",
    "    Review=Review.split()\n",
    "    \n",
    "    # we are removing the words like 'the','an'  and we are also stemming it \n",
    "    Review=[ps1.stem(word) for word in Review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # after cleaning we are just joning to make it as string\n",
    "    Review=' '.join(Review)\n",
    "    \n",
    "    # we will do this for every review and the review we got after cleaning we are appending to corpus\n",
    "    corpus1.append(Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=cv.transform(corpus1).toarray()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "           id  sentiment\n",
      "0  \"12311_10\"          1\n",
      "1    \"8348_2\"          0\n",
      "2    \"5828_4\"          1\n",
      "3    \"7186_2\"          1\n",
      "4   \"12128_7\"          1\n",
      "5    \"2913_8\"          0\n",
      "6    \"4396_1\"          0\n",
      "7     \"395_2\"          0\n",
      "8   \"10616_1\"          0\n",
      "9    \"9074_9\"          0\n"
     ]
    }
   ],
   "source": [
    "y_test=rf.predict(X_test)\n",
    "print(y_test.shape)\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n",
    "print(output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )  # getting a accuracy of 84.26% with max_fea=5000\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Dimensonality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried various small dimension and we will take the dimensions when the variance becomes greater than 90%\n",
    "500 components : 70.7%\n",
    "\n",
    "1000 components : 82.6 %\n",
    "\n",
    "2000 components : 92.85 %\n",
    "\n",
    "2500 componenets : 95.33%\n",
    "\n",
    "3000 componenents : 97.03 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2500, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2500)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,) 0.711072874909 0.82870601549 0.928427705475 0.95232176373\n"
     ]
    }
   ],
   "source": [
    "var=pca.explained_variance_ratio_.cumsum()\n",
    "print(var.shape,var[499],var[999],var[1999],var[2499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2500)\n"
     ]
    }
   ],
   "source": [
    "X_red=pca.transform(X)\n",
    "print(X_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2500)\n"
     ]
    }
   ],
   "source": [
    "X_test_red=pca.transform(X_test)\n",
    "print(X_test_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest=RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "randomforest.fit(X_red,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "           id  sentiment\n",
      "0  \"12311_10\"          1\n",
      "1    \"8348_2\"          0\n",
      "2    \"5828_4\"          1\n",
      "3    \"7186_2\"          1\n",
      "4   \"12128_7\"          1\n",
      "5    \"2913_8\"          0\n",
      "6    \"4396_1\"          0\n",
      "7     \"395_2\"          0\n",
      "8   \"10616_1\"          0\n",
      "9    \"9074_9\"          0\n"
     ]
    }
   ],
   "source": [
    "y_test=rf.predict(X_test)\n",
    "print(y_test.shape)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n",
    "print(output1.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output1.to_csv( \"Bag_of_Words_model1.csv\", index=False, quoting=3 )       # got same  accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after applying Dimensonality Reduction the accuracy has not changed that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Some New methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vr=VarianceThreshold()\n",
    "X_new=vr.fit_transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried to remove the columns whose variance is zero but you can see that there no colummns present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
